{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soroush-Khorami/CFAR10-Classification-with-CNNs/blob/main/Assignment01_DS04_S01_NLTK_SpaCy_RezaShokrzad_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b057fad5",
      "metadata": {
        "id": "b057fad5"
      },
      "source": [
        "# **Practice Assignment: NLP with NLTK & spaCy**\n",
        "\n",
        "* This assignment is part of the NLP Workshop on YouTube, which is free and open to the public.\n",
        "* **Lecturer: Reza Shokrzad.**\n",
        "*‚Äå [ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ÿ¨ŸÑÿ≥Ÿá ÿßŸàŸÑ ⁄©ŸÑÿßÿ≥](https://youtube.com/live/lDCoqQSc4ZE?feature=share)\n",
        "* [ÿ®ÿ±ŸÜÿßŸÖŸá ÿßÿ¨ÿ±ÿß€å€å ⁄©ŸÑÿßÿ≥ Ÿà ÿ¨ŸÑÿ≥ÿßÿ™](https://docs.google.com/spreadsheets/d/1SP3NJ9H7yp8sgof-zp_t4oxmdxjMdEgoL_mmCDvdUm4/edit?gid=0#gid=0)\n",
        "\n",
        "\n",
        "Welcome to this **Fill-in-the-Blanks NLP Assignment!** üéØ This exercise will help you solidify your understanding of **NLTK** and **spaCy** by filling in the missing parts of the code. Follow the instructions carefully, and make sure to test your solutions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa085b7f",
      "metadata": {
        "id": "aa085b7f"
      },
      "source": [
        "## **1. Working with Corpora & Lexical Resources**\n",
        "**Task:** Load and analyze texts from different corpora.\n",
        "- Use NLTK‚Äôs **Gutenberg** corpus to load the text of *Moby Dick*.\n",
        "- Tokenize it into words.\n",
        "- Count the top 10 most frequent words (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "757d9099",
      "metadata": {
        "id": "757d9099",
        "outputId": "89145640-ab81-4044-c372-00e9c88b7fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "# Load text\n",
        "text = gutenberg.raw('melville-moby_dick.txt')  # FILL THIS\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.isalnum() and word.lower() not in stopwords.words('english')]  # FILL THIS\n",
        "#*** list doest have lower() attribute so I deleted it. ***\n",
        "\n",
        "\n",
        "# Compute frequency distribution\n",
        "fdist = FreqDist(filtered_words)\n",
        "\n",
        "# Print top 10 words\n",
        "print(fdist.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2894d7a",
      "metadata": {
        "id": "f2894d7a"
      },
      "source": [
        "## **2. Tokenization Techniques**\n",
        "**Task:** Tokenize a given text using both **NLTK** and **spaCy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "85a9ac50",
      "metadata": {
        "id": "85a9ac50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2740ba-b611-4ee7-f347-fb0e55ad250a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Word Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n",
            "NLTK Sentence Tokens: ['SpaCy is fast!', 'However, NLTK provides flexibility in tokenization.']\n",
            "spaCy Tokens: ['SpaCy', 'is', 'fast', '!', 'However', ',', 'NLTK', 'provides', 'flexibility', 'in', 'tokenization', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"SpaCy is fast! However, NLTK provides flexibility in tokenization.\"\n",
        "\n",
        "# NLTK Tokenization\n",
        "nltk_word_tokens = word_tokenize(text)  # FILL THIS\n",
        "nltk_sent_tokens = sent_tokenize(text)  # FILL THIS\n",
        "\n",
        "# spaCy Tokenization\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "\n",
        "print(\"NLTK Word Tokens:\", nltk_word_tokens)\n",
        "print(\"NLTK Sentence Tokens:\", nltk_sent_tokens)\n",
        "print(\"spaCy Tokens:\", spacy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Regex Pattern Matching for Phone Number Detection**\n",
        "**Task:** Write a pattern using regex to find the phone nymber in the text."
      ],
      "metadata": {
        "id": "Sp7RCE4fIaVV"
      },
      "id": "Sp7RCE4fIaVV"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example 2: Phone Number Extraction\n",
        "text_phones = \"Call me at +1-202-555-0173 or reach our office at (415) 123-4567.\"\n",
        "phone_pattern = r\"\\+?\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"  # Regex for phone numbers\n",
        "\n",
        "phones = re.findall(phone_pattern, text_phones)\n",
        "print(\"Detected Phone Numbers:\", phones)\n"
      ],
      "metadata": {
        "id": "XeSzDZ2-Icdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f4736c-c72e-4560-dd00-d82c49cd0d38"
      },
      "id": "XeSzDZ2-Icdv",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Phone Numbers: ['+1-202-555-0173', '415) 123-4567']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Stopwords Filtering using NLTK**\n",
        "**Task:** Analyze movie reviews where stopwords are removed to focus on meaningful words."
      ],
      "metadata": {
        "id": "pdn94C-1cGgc"
      },
      "id": "pdn94C-1cGgc"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# üé¨ Sample Movie Review\n",
        "review = \"\"\"The movie was absolutely amazing! The cinematography was stunning, and the characters were incredibly well-developed.\n",
        "However, the storyline felt a bit predictable at times, and some scenes were unnecessarily long. Overall, a great experience!\"\"\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(review)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words(\"english\") and word.isalnum()]\n",
        "\n",
        "# Output results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nFiltered (No Stopwords):\", filtered_words)\n"
      ],
      "metadata": {
        "id": "qFGnnFOjcM23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e992065-44eb-47f7-cde2-019182201c77"
      },
      "id": "qFGnnFOjcM23",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'movie', 'was', 'absolutely', 'amazing', '!', 'The', 'cinematography', 'was', 'stunning', ',', 'and', 'the', 'characters', 'were', 'incredibly', 'well-developed', '.', 'However', ',', 'the', 'storyline', 'felt', 'a', 'bit', 'predictable', 'at', 'times', ',', 'and', 'some', 'scenes', 'were', 'unnecessarily', 'long', '.', 'Overall', ',', 'a', 'great', 'experience', '!']\n",
            "\n",
            "Filtered (No Stopwords): ['movie', 'absolutely', 'amazing', 'cinematography', 'stunning', 'characters', 'incredibly', 'However', 'storyline', 'felt', 'bit', 'predictable', 'times', 'scenes', 'unnecessarily', 'long', 'Overall', 'great', 'experience']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Stemming Methods using NLTK**\n",
        "**Task:** Analyze legal and scientific terms to observe how different stemming algorithms behave."
      ],
      "metadata": {
        "id": "7UezErpHcu3x"
      },
      "id": "7UezErpHcu3x"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "# ‚öñÔ∏è Sample Legal & Scientific Terms\n",
        "words = [\"arguing\", \"justification\", \"liable\", \"obligations\", \"classification\", \"microbiology\", \"evolutionary\", \"running\", \"happiness\"]\n",
        "\n",
        "# Initialize Stemmer Objects\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Apply Stemming\n",
        "porter_stems = [porter.stem(word) for word in words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in words]\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nPorter Stemmer Results:\", porter_stems)\n",
        "print(\"\\nLancaster Stemmer Results:\", lancaster_stems)\n"
      ],
      "metadata": {
        "id": "Kpvzkd31d6Na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1848d945-e340-4c80-d4f2-69970bd1efbd"
      },
      "id": "Kpvzkd31d6Na",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['arguing', 'justification', 'liable', 'obligations', 'classification', 'microbiology', 'evolutionary', 'running', 'happiness']\n",
            "\n",
            "Porter Stemmer Results: ['argu', 'justif', 'liabl', 'oblig', 'classif', 'microbiolog', 'evolutionari', 'run', 'happi']\n",
            "\n",
            "Lancaster Stemmer Results: ['argu', 'just', 'liabl', 'oblig', 'class', 'microbiolog', 'evolv', 'run', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **Lemmatization Strategies using NLTK & spaCy**\n",
        "\n",
        "### NLTK‚Äôs WordNetLemmatizer\n",
        "**Task:** Lemmatize a political news headline to show how lemmatization helps retain the correct part of speech (POS) while normalizing words."
      ],
      "metadata": {
        "id": "ffIhEdRJeoI4"
      },
      "id": "ffIhEdRJeoI4"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# üì∞ Sample News Headline\n",
        "headline = \"The senators debated the increasing regulations affecting technology companies.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(headline)\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply Lemmatization (default without POS tagging)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "o4LlZqSxe086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b979204-2b9e-4f19-99bf-80d15dcbe5e8"
      },
      "id": "o4LlZqSxe086",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'senators', 'debated', 'the', 'increasing', 'regulations', 'affecting', 'technology', 'companies', '.']\n",
            "\n",
            "Lemmatized Words: ['The', 'senator', 'debated', 'the', 'increasing', 'regulation', 'affecting', 'technology', 'company', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy‚Äôs Built-in Lemmatizer"
      ],
      "metadata": {
        "id": "C61P_pdvfJlH"
      },
      "id": "C61P_pdvfJlH"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the same headline\n",
        "doc = nlp(headline)\n",
        "\n",
        "# Apply Lemmatization\n",
        "spacy_lemmatized = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"\\nspaCy Lemmatized Words:\", spacy_lemmatized)\n"
      ],
      "metadata": {
        "id": "4qPNWbqafWt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a899b5d2-42ed-4b39-8ee2-587fe850d03b"
      },
      "id": "4qPNWbqafWt3",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "spaCy Lemmatized Words: ['the', 'senator', 'debate', 'the', 'increase', 'regulation', 'affect', 'technology', 'company', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **Parsing & Chunking using NLTK**\n",
        "\n",
        "**Task:** Analyze legal contracts and job descriptions where parsing and chunking help extract meaningful phrases like noun phrases (NPs) or verb phrases (VPs)."
      ],
      "metadata": {
        "id": "nf-YB5yjfdl5"
      },
      "id": "nf-YB5yjfdl5"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìú Task: Extracting Key Phrases from Legal & Job Documents\n",
        "import nltk\n",
        "\n",
        "# nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "\n",
        "# üìú Sample Legal Contract Text\n",
        "contract_text = \"The tenant shall pay the monthly rent before the 5th of each month.\"\n",
        "\n",
        "# Tokenize & POS Tagging\n",
        "words = nltk.tokenize.word_tokenize(contract_text)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Define a Chunking Grammar for Noun Phrases (NP)\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "\n",
        "# Apply Chunking\n",
        "chunk_parser = nltk.RegexpParser(grammar)\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Display Results\n",
        "print(\"Chunked Tree:\")\n",
        "tree.pretty_print()\n"
      ],
      "metadata": {
        "id": "t9e4fOIuft4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d14128a-f6ad-4400-f412-3b2d6ba9aae7"
      },
      "id": "t9e4fOIuft4n",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked Tree:\n",
            "                                               S                                                                     \n",
            "    ___________________________________________|__________________________________________________________            \n",
            "   |       |        |       |      |      |    |          NP                      NP                      NP         \n",
            "   |       |        |       |      |      |    |     _____|______         ________|_________         _____|_____      \n",
            "shall/MD pay/VB before/IN the/DT 5th/CD of/IN ./. The/DT     tenant/NN the/DT monthly/JJ rent/NN each/DT     month/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **Exploring Hyponyms & Hypernyms using WordNet (NLTK)**\n",
        "\n",
        "**Task:** Hyponyms (specific terms) and hypernyms (general terms) in scientific and business domains, where hierarchical relationships between words are essential."
      ],
      "metadata": {
        "id": "GTmtGRjofqpB"
      },
      "id": "GTmtGRjofqpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# üîç Task: Explore Word Relationships in Science & Business\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# ü¶Å Find Hypernyms & Hyponyms for \"lion\"\n",
        "word = \"lion\"\n",
        "synset = wordnet.synsets(word)[0]  # Selecting the first synset\n",
        "\n",
        "# Hypernyms (More General Category)\n",
        "hypernyms = synset.hypernyms()\n",
        "print(f\"Hypernyms (More General Concept) of '{word}':\")\n",
        "print([hypernym.name().split('.')[0] for hypernym in hypernyms])\n",
        "\n",
        "# Hyponyms (More Specific Types)\n",
        "hyponyms = synset.hyponyms()\n",
        "print(f\"\\nHyponyms (More Specific Types) of '{word}':\")\n",
        "print([hyponym.name().split('.')[0] for hyponym in hyponyms])"
      ],
      "metadata": {
        "id": "EELbOY0dgjsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381daa80-4413-426d-990d-e73cddeabe70"
      },
      "id": "EELbOY0dgjsK",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('dog.n.01')\n",
            "dog.n.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb5a06b",
      "metadata": {
        "id": "fdb5a06b"
      },
      "source": [
        "## **9. Named Entity Recognition (NER) with spaCy**\n",
        "**Task:** Extract named entities from a complex sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "dc7654be",
      "metadata": {
        "id": "dc7654be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efeb0eca-faa6-4110-c649-4b67c0ec4dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "1969 -> DATE\n",
            "Neil Armstrong -> PERSON\n",
            "first -> ORDINAL\n",
            "Moon -> PERSON\n",
            "Apollo 11 -> LAW\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\"\n",
        "\n",
        "doc = nlp(text)  # FILL THIS\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}